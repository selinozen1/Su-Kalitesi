# -*- coding: utf-8 -*-
"""SuKalitesiTahmini.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DkI1-1RcxS0OmbWwc8mDn898Vp6XvGT1
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import missingno as msno
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV, RepeatedStratifiedKFold, train_test_split
from sklearn.metrics import confusion_matrix, precision_score
from sklearn import tree

df = pd.read_csv('water_potability.csv')
df

describe = df.describe()
describe

df.info()

d = pd.DataFrame(df["Potability"].value_counts())
d

fig = px.pie(d, values = "count", names=["Not Potable", "Potable"], hole=0.35, opacity=0.8, labels={"Label" : "Potability", "Potability":"Number of Samples"})
fig.update_layout(title = dict(text = "Pie Chart of Potability Feature"))
fig.update_traces(textposition='outside', textinfo='percent+label')
fig.show()
fig.write_html("potability_pie_chart.html")

sns.clustermap(df.corr(), cmap="vlag", dendrogram_ratio=(0.1, 0.2), annot=True, linewidths=0.8, figsize=(10, 10))
plt.show()

non_potable = df.query("Potability == 0")
potable = df.query("Potability == 1")

plt.figure(figsize=(15, 15))
for ax, col in enumerate(df.columns[:9]):
  plt.subplot(3, 3, ax+1)
  plt.title(col)
  sns.kdeplot(x = non_potable[col], label="Not Potable")
  sns.kdeplot(x = potable[col], label="Potable")
  plt.legend()
plt.tight_layout()

msno.matrix(df)
plt.show()

print(df.isnull().sum())

df["ph"].fillna(value= df["ph"].mean(), inplace=True)
df["Sulfate"].fillna(value= df["Sulfate"].mean(), inplace=True)
df["Trihalomethanes"].fillna(value= df["Trihalomethanes"].mean(), inplace=True)
print(df.isnull().sum())

X = df.drop("Potability", axis=1)
y = df["Potability"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

x_train_max = np.max(X_train)
x_train_min = np.min(X_train)
X_train = (X_train - x_train_min) / (x_train_max - x_train_min)
X_test = (X_test - x_train_min) / (x_train_max - x_train_min)

models = [("DTC", DecisionTreeClassifier(max_depth=3)), ("RFC", RandomForestClassifier())]
finalResult = []  #score
cmList = []    # confusion matrix list
for name, model in models:
  model.fit(X_train, y_train)
  y_pred = model.predict(X_test)
  score = precision_score(y_test, y_pred)
  finalResult.append((name, score))
  cm = confusion_matrix(y_test, y_pred)
  cmList.append((name, cm))

print(finalResult)
for name, i in cmList:
  plt.figure()
  sns.heatmap(i, annot=True, linewidths=0.8, fmt=".0f")
  plt.title(name)
  plt.show()

dt_clf = models[0][1]
plt.figure(figsize=(20, 20))
tree.plot_tree(dt_clf, feature_names=df.columns.to_list()[:-1], class_names=["0", "1"], filled=True, precision=5)
plt.show()

model_params = {
    "Random Forest":
    {
        "model": RandomForestClassifier(),
        "params":
        {
            "n_estimators": [10, 50, 100],
            "max_features": ["auto", "sqrt", "Log2"],
            "max_depth": list(range(1, 21, 3)),
        }
    }
}
cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2)
scores = []

for model_name, params in model_params.items():
  rs = RandomizedSearchCV(params["model"], params["params"], cv=cv, n_iter=10)
  rs.fit(X_train, y_train)
  scores.append([model_name, dict(rs.best_params_), rs.best_score_])

print(scores)


#[['Random Forest', {'n_estimators': 50, 'max_features': 'sqrt', 'max_depth': 10}, np.float64(0.6456603019664925)]]
#[['Random Forest', {'n_estimators': 100, 'max_features': 'sqrt', 'max_depth': 4}, np.float64(0.6282097021244208)]]